\chapter{Implementation and Testing}
The goal of the project is to implement a machine-learning model that has the ability to predict inflation.
In order to find the most suitable model, several different models were implemented and their results were compared.
This chapter of the report will cover how the models were implemented, which models were implemented, how the models were tuned and tested, as well as some of the issues encountered in the implementation process.
This chapter will not cover the results of the models, their comparisons, or their evaluations.
\section{Tools}
\subsection{Coding Environemnt}
The IDE used for this project was Jupyter Notebook provided through the use of Anaconda.
Anaconda is a platform that includes a packet manager, this makes it easy to control packages and their dependencies that are installed in an environment.
Each environment in Anaconda is isolated which allows version control and preventing conflicts between the installed versions and packages of different projects.
This is valuable when working on multiple projects simultaneously.
Furthermore, Anaconda provides Jupyter Notebook integration which is beneficial as it is a common IDE for data science and has an active community making it easy to learn.
\subsection{Visualisation}
Jupyter Notebook provides visualisation as output from individual cells.
This was useful for debugging throughout the project.
Both Matplotlib and Seaborn were used for further graphical visualisation.
\subsection{libraries}
The project's code was written in Python and several libraries were used to assist the coding process.\\

NumPy\cite{harris2020array} and Pandas\cite{reback2020pandas} - These libraries were used for data manipulation and cleaning.
NumPy provides several tools for dealing with multidimensional arrays. 
Pandas is built upon NumPy and is used for dealing with tabular data (data that is organised into a table with rows and columns).
Pandas also has built-in tools for dealing with time-series data which is useful as inflation is time-series data.\\

TensorFlow\cite{tensorflow2015-whitepaper} and Keras\cite{chollet2015keras} - TensorFlow is a platform developed by Google for training machine learning models.
It makes defining a model straightforward while still providing enough control and flexibility of the model's structure, inputs, and outputs. 
Using pre-written functions and nodes that were written by a reliable source both saves time and resources during development.
Keras is built on top of TensorFlow and aims to provide a simplified and more user-friendly interface for creating and training machine learning models.\\

SciKit-Learn\cite{scikit-learn} - Sklearn is a machine learning library that provides a wide range of easy-to-implement machine learning models and metrics.
Its extensive documentation makes it easy to learn and implement.\\

Matplotlib\cite{Hunter:2007} and Seaborn\cite{Waskom2021} - Matplotlib and Seaborn were used for all graphical data visualisation.
These libraries provide a plethora of graphing options along with plenty of community support.

Sktime\cite{Loning2022-mg} - Sktime is a library for machine learning with time series data. 
This was used to implement the univariate time series forecasting of inflation.

\section{Data Preperation}
Features relating to inflation were sourced from the OECD Databank and Monthly CPI indicators were taken from the World Bank's online statistics.
All of this data is relating to the United Kingdom's general economic indicators.
The data was compiled into a tabular format to facilitate the use of the Pandas library and to streamline further data manipulation tasks.
By using a pandas dataframe several functions can be used to quickly assess the state of the data such as the head or shape functions.
This is useful in the context of machine learning as model layers require inputs of certain types or shapes.
\subsection{Normalising the Data}
Normalisation, also known as feature scaling, is used to rescale data between a given range. 
The importance of normalisation is due to the fact that many of the features may have varying ranges, this could lead to features with a large range having a greater impact on the result.
By adjusting all of the features to range between the same values, no single feature will dominate and all features should contribute equally to the final result.
Additionally, normalisation increases the speed at which gradient descent converges, which will decrease the runtime of the models.\cite{IoffeS15}
Normalising the data is often done either in the range [0,1] or [-1,1].
The dataset for this project was normalised between the range of [0,1] with the use of the MinMaxScaler from the sklearn library.
There are several benefits to scaling to a small range such as reducing the impact of outliers and increasing the speed of the algorithms.\\
\subsection{Splitting the Data into Training and Testing Splits}
It is common practice to split a dataset into training and testing splits.
The training split is then used for training the machine learning model and the testing split is used for testing the model's performance on unseen data.
If the model was tested on data it had already been trained on it would become difficult to evaluate the model's generalization (its ability to perform on new/unseen data).
The difficulty in deciding the size of the training and testing splits lies in the fact that too small a training split and our model may not perform well but too small a testing split and our performance metrics will have a greater variance. 
The project dataset was split into 80\% training data and 20\% testing data following the advice from the article: A scaling law for the validation-set training-set size ratio by Isabelle Guyon\cite{traintestsplit}.


As the dataset is time series data it is likely to contain some temporal dependency.
Temporal dependency is when data contains stronger associations between events that happened within the same time period.
To preserve the temporal dependency of the data
The dataset was then split using the "train\textunderscore test\textunderscore split" function from sklearn.


\section{The Machine Learning Models}
Five different models were implemented and tested for the predictive ability on the same data set.
The different machine learning models were linear regression, random forest, support vector regression, long short-term memory network, and a custom artificial neural network.
The reason for these five models was because they are some of the most common models used for regression analysis and, as found in the literature, many of the models are used for economic indicator analysis (such as stock predictions).
Three of the five models were implemented with the Scikit-learn library.
These were linear regression, random forest, and support vector regression.
The remaining two models were implemented through the use of TensorFlow and Keras.

\subsection{Univariate Implementation}
Before Implementing each model with the full dataset that contained twenty different features, some of the simple models were implemented.
The models that were implemented were linear regression, random forest with 20 trees (as this produced the best results), and support vector regression.
This was a quick implementation to test if predicting inflation solely based on its past values would suffice.
As expected, the results of this experiment were lackluster likely due to inflation's complexity and the fact that many outside factors affect it.
Of the three models, random forest regression produced the best results although the quality of the predictions was still not satisfactory. 


\subsection{Multivariate Analysis}
On the topic of outside factors affecting inflation, the curated dataset that was used for this project contained 20 different features, each with monthly data dating back to 1972 at the earliest and 1987.
The OECD features are macroeconomic indicators that focus on broad trends from the UK with global implications.
Macroeconomic indicators are better predictors of inflation as inflation happens on a macro scale.
