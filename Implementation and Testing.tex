\chapter{Implementation and Testing}
The goal of the project is to implement a machine-learning model that has the ability to predict inflation.
In order to find the most suitable model, several different models were implemented and their results were compared.
This chapter of the report will cover how the models were implemented, which models were implemented, how the models were tuned and tested, as well as some of the issues encountered in the implementation process.
This chapter will not cover the results of the models, their comparisons, or their evaluations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Tools}
\subsection{Coding Environemnt}
The IDE used for this project was Jupyter Notebook provided through the use of Anaconda.
Anaconda is a platform that includes a packet manager, this makes it easy to control packages and their dependencies that are installed in an environment.
Each environment in Anaconda is isolated which allows version control and preventing conflicts between the installed versions and packages of different projects.
This is valuable when working on multiple projects simultaneously.
Furthermore, Anaconda provides Jupyter Notebook integration which is beneficial as it is a common IDE for data science and has an active community making it easy to learn.

\subsection{Visualisation}
Jupyter Notebook provides visualisation as output from individual cells.
This was useful for debugging throughout the project.
Both Matplotlib and Seaborn were used for further graphical visualisation.

\subsection{Libraries}
The project's code was written in Python and several libraries were used to assist the coding process.\\

NumPy\cite{harris2020array} and Pandas\cite{reback2020pandas} - These libraries were used for data manipulation and cleaning.
NumPy provides several tools for dealing with multidimensional arrays. 
Pandas is built upon NumPy and is used for dealing with tabular data (data that is organised into a table with rows and columns).
Pandas also has built-in tools for dealing with time-series data which is useful as inflation is time-series data.\\

TensorFlow\cite{tensorflow2015-whitepaper} and Keras\cite{chollet2015keras} - TensorFlow is a platform developed by Google for training machine learning models.
It makes defining a model straightforward while still providing enough control and flexibility of the model's structure, inputs, and outputs. 
Using pre-written functions and nodes that were written by a reliable source both saves time and resources during development.
Keras is built on top of TensorFlow and aims to provide a simplified and more user-friendly interface for creating and training machine learning models.\\

SciKit-Learn\cite{scikit-learn} - Sklearn is a machine learning library that provides a wide range of easy-to-implement machine learning models and metrics.
Its extensive documentation makes it easy to learn and implement.\\

Matplotlib\cite{Hunter:2007} and Seaborn\cite{Waskom2021} - Matplotlib and Seaborn were used for all graphical data visualisation.
These libraries provide a plethora of graphing options along with plenty of community support.

Sktime\cite{Loning2022-mg} - Sktime is a library for machine learning with time series data. 
This was used to implement the univariate time series forecasting of inflation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data Preperation}
Features relating to inflation were sourced from the OECD Databank and Monthly CPI indicators were taken from the World Bank's online statistics.
All of this data is relating to the United Kingdom's general economic indicators.
The data was compiled into a tabular format to facilitate the use of the Pandas library and to streamline further data manipulation tasks.
By using a pandas dataframe several functions can be used to quickly assess the state of the data such as the head or shape functions.
This is useful in the context of machine learning as model layers require inputs of certain types or shapes.

\subsection{Normalising the Data}
Normalisation, also known as feature scaling, is used to rescale data between a given range. 
The importance of normalisation is due to the fact that many of the features may have varying ranges, this could lead to features with a large range having a greater impact on the result.
By adjusting all of the features to range between the same values, no single feature will dominate and all features should contribute equally to the final result.
Additionally, normalisation increases the speed at which gradient descent converges, which will decrease the runtime of the models.\cite{IoffeS15}
Normalising the data is often done either in the range [0,1] or [-1,1].
The dataset for this project was normalised between the range of [0,1] with the use of the MinMaxScaler from the sklearn library.
There are several benefits to scaling to a small range such as reducing the impact of outliers and increasing the speed of the algorithms.\\

\subsection{Splitting the Data into Training and Testing Splits}
It is common practice to split a dataset into training and testing splits.
The training split is then used for training the machine learning model and the testing split is used for testing the model's performance on unseen data.
If the model was tested on data it had already been trained on it would become difficult to evaluate the model's generalization (its ability to perform on new/unseen data).
The difficulty in deciding the size of the training and testing splits lies in the fact that too small a training split and our model may not perform well but too small a testing split and our performance metrics will have a greater variance. 
The project dataset was split into 80\% training data and 20\% as this is a common split and the models will not utilize a validation set.
As the dataset is made up of time series data, splitting the data carelessly can lead to suboptimal results.
This is because time series data often has temporal dependencies.
Temporal dependency is when data contains stronger associations between events that happened within the same time period.
To preserve the temporal dependency of the data the dataset will not be shuffled (randomised) when it is split into the training and testing sets.
The dataset was split using the "train\textunderscore test\textunderscore split" function from sklearn.

\subsection{Inflation's Historic Outliers}
The history of inflation is riddled with outliers caused by unexpected or unprecidented events.
Covid-19, or Russia's declaration of war on Ukraine to name a few.
However, the UK's largest spike of inflation in recent history happened in the 1970s due to spikes in oil prices and a steep rise in wages. 
The result of which is inflation reaching as high as 25%. 
Although the purpose of predicting inflation is to also be able to foresee such large events, including an outlier of this magnitude in our training data will likely have a negative impact on our models predictive ability as well as its generalization. 
\begin{figure}[H]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{totalCPI.png}
        \label{fig:totalCPI}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{limitedCPI.png}
        \label{fig:limitedCPI}
    \end{minipage}
    \caption[History of CPI vs History of CPI Excluding the First 150 Datapoints.]{History of CPI vs History of CPI Excluding the First 150 Datapoints.}
\end{figure}
As such, after training and testing the models on the entire dataset and comparing the results to those of models trained on a subset of the data, the decision was made to remove the first 150 datapoints from the dataset.
This would remove the large outlying time period of the 70s whilest still retaining some peaks in inflation in order to give the model a realistic expectation for how inflation may peak again.
Removing the first 150 datapoints reduces the size of the dataset by a significant portion, however, the subset of data still produced more accurate results when testing the models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Machine Learning Models}
Five different models were implemented and tested for the predictive ability on the same data set.
The different machine learning models were linear regression, random forest, support vector regression, long short-term memory network, and a custom artificial neural network.
The reason for these five models was because they are some of the most common models used for regression analysis and, as found in the literature, many of the models are used for economic indicator analysis (such as stock predictions).
Three of the five models were implemented with the Scikit-learn library.
These were linear regression, random forest, and support vector regression.
The remaining two models were implemented through the use of TensorFlow and Keras.

\subsection{Univariate Implementation}
Univariate analysis is the practice of analysising a single variable to understand its patterns and characteristics.
Univariate analysis does not involve relationships or causes from other variables.
Before Implementing each machine learning model with the full dataset, a subset of the models were trained and tested using inflation as the only variable.
The models that were implemented were linear regression, random forest regression (using 20 trees), and support vector regression.
This was a quick implementation to test if predicting inflation solely based on its past values would suffice.
As expected, the results produced were lackluster, likely due to inflation's complexity and the fact that many outside factors affect it.
Of the three models, random forest regression produced the best results although the quality of the predictions was still not satisfactory. 

\subsection{Multivariate Implementation}
Multivariate analysis is the practice of compiling multiple unique data variables in order to create a more holistic forecast of a related variable.
\subsubsection{The Features and Dataset}
The curated dataset that was used for this project contained 20 different features, each with monthly data dating back to 1972 at the earliest and 1987.
The OECD features are macroeconomic indicators that focus on broad trends from the UK with global implications.
Macroeconomic features were selected as inflation is itself happens on a macro scale thus it stands to reason that macro features will likely predict inflation better than micro ones.

Upon analyis of the features selected for the dataset (as seen in figures \ref{fig:corr} and \ref{fig:hist}) very few features share a strong correlation or similar distribution to inflation.
This could be worrying as it may indicate that, although on an intuitive surface level that many of these features should contribute to inflation, the features may not possess a strong ability to predict inflation.
However, the lack of obvoius correlation shown in many points of the heatmap figure does not completely rule out the fact that two points correlate or have the abilty to aid in predicting one another.
This is due to the fact that the heatmap figure was generate using a pairwise pearson correlation function.
Although pairwise pearson correlation can often give a good representation of the correlation of two features it may not always be accurate.
Pearson correlation attempts to draw a line of best fit between two features, meaning that if the two features do not share a linear correlation then pearson correlation may not produce a strong result.
As the relationship between inflation and its causes are extremely complex, with many different factors contributing to the value of inflation in varying amounts, it is unlikely that inflation will have a strong linear correlation with the features that contribute it.
So despite the fact that the results of the heatmap were not ideal, this does not mean that the features are innapropriate to use in our models.

\subsubsection{The Models Used for Multivariate Analysis}
All five of the previously stated models (see 3.3.1 Model Selection) were implemented for multivariate analysis.
Linear regression, random forest regression, and support vector regression were implemented with the help of the respective sklearn libraries.
Long short-term memory network (LSTM) and the custom neural network(NN) were implemented with the use of TensorFlow and Keras.
The custom neural network, somewhat expectedly, produced the best predictions.
The results of each model will be explored further in the following chapter.

\subsection{Model Architecture}
The architecture of a model is the structure of the model including the types of layers, number of nodes and connections between layers as well as the input and output of the model.


\subsubsection{Hyperparameter Optimisation}
Both the LSTM and custom NN underwent hyperparameter optimisation in order to produce the best performance for each model.
The hyperparameter optimisation was done through a grid search technique.
The grid search method is done by exhaustively searching a manually specified set of hyperparameters.
The purpose for optimising the hyperparameters in this was is to ensure that the hyperparameters are the best possible values within the given subset.
The downside to optimising hyperparameters using a grid search technique is that it is computationally expensive.
This is because the model needs to be trained and tested on every possible combination of hyperparameters in order to asertain which combination is optimal.
Grid search is not the only algorithm for hyperparameter optimisation, alternatives to the grid search method are a random search algorithm and a gradient descent algorithm.
Random search randomly samples combinations of parameters so it is less computationally expensive but it is unlikely to produce an optimal combination.
Gradient descent optimisation updates hyperparameters to minimise the result of a loss function but there is a chance of the algorithm getting stuck in a local minima which may not be the global mimum of the function, thus producing a sub optimal result.
Because of these factors, the grid search technique was chosen due to the fact that it is more comprehensive than the other algorithms and is guaranteed to produce the optimal combination of hyperparameters from the given set. 