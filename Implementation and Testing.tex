\chapter{Implementation and Testing}
The goal of this project is to implement several machine-learning models that have the ability to predict inflation.
The results of each model will be compared in order to find the most suitable model for the task.
This chapter of the report will cover: 
which models were implemented, how they were implemented, how the models were tuned and tested, the results produced by the models, and some of the issues encountered in the implementation process.
This chapter will not contain the evaluation of the models' results or a comparison of the results.

\section{Tools}
\subsection{Coding Environemnt}
The IDE used for this project was Jupyter Notebook provided through the use of Anaconda.
Anaconda is a platform that includes a packet manager, this makes it easy to control packages and their dependencies that are installed in an environment.
Each environment in Anaconda is isolated which allows version control and preventing conflicts between the installed versions and packages of different projects.
This is valuable when working on multiple projects simultaneously.
Furthermore, Anaconda provides Jupyter Notebook integration which is beneficial as it is a common IDE for data science and has an active community making it easy to learn.

\subsection{Visualisation}
Jupyter Notebook provides visualisation as output from individual cells.
This was useful for debugging throughout the project.
Both Matplotlib and Seaborn were used for further graphical visualisation.

\subsection{Libraries}
The project's code was written in Python and several libraries were used to assist the coding process.\\

NumPy\cite{harris2020array} and Pandas\cite{reback2020pandas} - These libraries were used for data manipulation and cleaning.
NumPy provides several tools for dealing with multidimensional arrays. 
Pandas is built upon NumPy and is used for dealing with tabular data (data that is organised into a table with rows and columns).
Pandas also has built-in tools for dealing with time-series data which is useful as inflation is time-series data.\\

TensorFlow\cite{tensorflow2015-whitepaper} and Keras\cite{chollet2015keras} - TensorFlow is a platform developed by Google for training machine learning models.
It makes defining a model straightforward while still providing enough control and flexibility of the model's structure, inputs, and outputs. 
Using pre-written functions and nodes that were written by a reliable source both saves time and resources during development.
Keras is built on top of TensorFlow and aims to provide a simplified and more user-friendly interface for creating and training machine learning models.\\

SciKit-Learn\cite{scikit-learn} - Sklearn is a machine learning library that provides a wide range of easy-to-implement machine learning models and metrics.
Its extensive documentation makes it easy to learn and implement.\\

Matplotlib\cite{Hunter:2007} and Seaborn\cite{Waskom2021} - Matplotlib and Seaborn were used for all graphical data visualisation.
These libraries provide a plethora of graphing options along with plenty of community support.\\

Sktime\cite{Loning2022-mg} - Sktime is a library for machine learning with time series data. 
This was used to implement the univariate time series forecasting of inflation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data Preperation}
Features relating to inflation were sourced from the OECD Databank and Monthly CPI indicators were taken from the World Bank's online statistics.
All of this data is relating to the United Kingdom's general economic indicators.
The data was compiled into a tabular format to facilitate the use of the Pandas library and to streamline further data manipulation tasks.
By using a pandas dataframe several functions can be used to quickly assess the state of the data such as the head or shape functions.
This is useful in the context of machine learning as model layers require inputs of certain types or shapes.

\subsection{Maintaining Time Consistency}
Using time series data in a machine learning model requires additional attention compared to regular data, especially when testing the model.
General practice in machine learning is to split the dataset into arrays for training and testing and into X and y (with X being the data that predicts the target data y).
However, in time series forecasting the data is created or released over time.
This means that using X data from a specific year to predict the y value from the same year is redundant as target data is usually released around the same time as the X data, meaning that it is already available.
Instead, the aim of time series forecasting should be to use past data to predict future data.
In creating a dataset for a machine learning model this takes the form of shifting the target forward in time so that the X data from the current year is being used to predict the y data from the next year.
Often, the further forward the target data is shifted the less accurate the model becomes.
In this project, the target values of inflation were shifted 12 months ahead of the X data.
This means that the model uses data from the current year to predict the inflation values of the next year.
Additionally, a feature was implemented in the project where the user can adjust the number of months that inflation is shifted to provide additional predictive flexibility if needed.

\subsection{Normalising the Data}
Normalisation, also known as feature scaling, is used to rescale data between a given range. 
The importance of normalisation is due to the fact that many of the features may have varying ranges, this could lead to features with a large range having a greater impact on the result.
By adjusting all of the features to range between the same values, no single feature will dominate and all features should contribute equally to the final result.
Additionally, normalisation increases the speed at which gradient descent converges, which will decrease the runtime of the models.\cite{IoffeS15}
Normalising the data is often done either in the range [0,1] or [-1,1].
The dataset for this project was normalised between the range of [0,1] with the use of the MinMaxScaler from the Sklearn library.
There are several benefits to scaling to a small range such as reducing the impact of outliers and increasing the speed of the algorithms.\\

\subsection{Splitting the Data into Training and Testing Splits}
It is common practice to split a dataset into training and testing splits.
The training split is then used for training the machine learning model and the testing split is used for testing the model's performance on unseen data.
If the model was tested on data it had already been trained on it would become difficult to evaluate the model's generalization (its ability to perform on new/unseen data).
The difficulty in deciding the size of the training and testing splits lies in the fact that too small a training split and the model may not perform well but too small a testing split and the performance metrics will have a greater variance. 
The project dataset was split into 70\% training data and 30\% testing data as this is a common split.
The models will not utilize a validation set.
As the dataset is made up of time series data, splitting the data carelessly can lead to suboptimal results.
This is because time series data often has temporal dependencies.
Temporal dependency is when data contains stronger associations between events that happened within the same time period.
To preserve the temporal dependency of the data the dataset will not be shuffled (randomised) when it is split into the training and testing sets.
The dataset was split using the "train\textunderscore test\textunderscore split" function from Sklearn.

\subsection{Inflation's Historic Outliers}
The history of inflation is riddled with outliers caused by unexpected or unprecedented events.
Covid-19, or Russia's declaration of war on Ukraine to name a few.
However, the UK's largest spike of inflation in recent history happened in the 1970s due to spikes in oil prices and a steep rise in wages. 
The result of this is inflation reaching as high as 25\%. 
Although the purpose of predicting inflation is to also be able to foresee such large events, including an outlier of this magnitude in our training data will likely harm our model's predictive ability as well as its generalization. 
\begin{figure}[H]
    \centering
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{totalCPI.png}
        \label{fig:totalCPI}
    \end{minipage}%
    \begin{minipage}{.5\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{limitedCPI.png}
        \label{fig:limitedCPI}
    \end{minipage}
    \caption[History of CPI vs History of CPI Excluding the First 150 Datapoints.]{History of CPI vs History of CPI Excluding the First 150 Datapoints.}
    \label{fig:totalCPIvsLimitedCPI}
\end{figure}
The graphs in figure \ref{fig:totalCPIvsLimitedCPI} indicate the difference in magnitude from the peaks in the entire dataset vs the peaks in the dataset that omit the first 150 datapoints.
As such, after training and testing the models on the entire dataset and comparing the results to those of models trained on a subset of the data, the decision was made to remove the first 150 data points from the dataset.
This would remove the large outlying period of the 70s while still retaining some peaks in inflation in order to give the model a realistic expectation for how inflation may peak again.
Removing the first 150 data points reduces the size of the dataset by a significant portion, however, the subset of data still produced more accurate results when testing the models.

\section{The Machine Learning Models}
Five different models were implemented and tested for the predictive ability on the same data set.
The different machine learning models were linear regression, random forest regression, support vector regression, long short-term memory network, and a feedforward neural network.
The reason for these five models was because they are some of the most common models used for regression analysis and, as found in the literature, many of the models are used for economic indicator analysis (such as stock predictions).
Three of the five models were implemented with the Scikit-learn library.
These were linear regression, random forest, and support vector regression.
The remaining two models were implemented through the use of TensorFlow and Keras.

\section{Univariate Implementation}
Univariate analysis is the practice of analysing a single variable to understand its patterns and characteristics.
Univariate analysis does not involve relationships or causes from other variables.
Before Implementing each machine learning model with the full dataset, a subset of the models were trained and tested using inflation as the only variable.
The models that were implemented were linear regression, random forest regression (using 20 trees), and support vector regression.
This was a quick implementation to test if predicting inflation solely based on its past values would suffice.

As expected, the results produced were lackluster, likely due to inflation's complexity and the fact that many outside factors affect it.
Of the three models, random forest regression produced the best results although the quality of the predictions was still not satisfactory. 

\section{Multivariate Implementation}
Multivariate analysis is the practice of compiling multiple unique data variables in order to create a more holistic forecast of the target variable.

\subsection{The Features and Dataset}
The curated dataset that was used for this project contained 20 different features, each with monthly data dating back to 1972 at the earliest and 1987 at the latest.
The OECD features are macroeconomic indicators that focus on broad trends from the UK with global implications.
Macroeconomic features were selected as inflation happens on a macro scale thus it stands to reason that macro features will predict inflation better than micro ones.

Upon analysis of the features selected for the dataset (as seen in figures \ref{fig:hist} and \ref{fig:corr}) very few features share a strong correlation or similar distribution to inflation.
This could be worrying as it may indicate that, although on an intuitive surface level, many of these features should contribute to inflation, the features may not possess a strong ability to predict inflation.
However, this is not the case as explored in the previous design chapter, the Pearson correlation calculated in the correlation matrix and the distribution of data shown in the histograms are not the be-all and end-all of value forecasting.
This is reiterated further in the Granger causality tests carried out in the Design chapter as several features are shown to have the potential to predict inflation.
Therefore, as there is no indication as to whether or not any of the features are redundant, all of the features will be kept in the dataset for training the models.

\subsection{The Models Used for Multivariate Analysis}
All five of the previously stated models (see 3.3.1 Model Selection) were implemented for multivariate analysis.
Linear regression, random forest regression, and support vector regression were implemented with the help of the respective Sklearn libraries.
The long short-term memory network (LSTM) and the feedforward neural network (FNN) were implemented with the use of TensorFlow and Keras.
Of the models implemented, the feedforward neural network produced the best results.
The results of each model will be explored further in the following chapter.

\subsection{Model Architecture}
The architecture of a model is the structure of the model including the types of layers, number of nodes, and connections between layers as well as the input and output of the model.
This subsection will cover the architecture of the LSTM model and the feedforward neural network model.
The linear regression, random forest regression, and support vector regression models will not be covered in this section as they are algorithm-based models. 
The parameters used for these three models were the default parameters outlined in the Sklearn documentation.

\subsubsection{Feedforward Neural Network Model Architecture}
An artificial neural network (ANN) is a machine-learning model inspired by the human brain.
ANNs feed data through layers of interconnected artificial neurons (nodes) in order to compute complex problems.
ANNs can vary in size and shape as well as the different types of nodes they contain.

After some research and experimentation with various structures, the structure shown in figure \ref{fig:ANNSummary} was implemented for the feedforward neural network.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{ANNSummary.png}
    \caption[FNN Model Architecture Summary.]{FNN Model Architecture Summary.}
    \label{fig:ANNSummary}
\end{figure}
Figure \ref{fig:ANNSummary} shows the structure of the FNN model, including each layer's output shape, and parameters.
The layer column of the figure indicates which types of neural network layers were implemented.
The output shape column states the shape of the data, this also indicates the number of nodes in a layer.
Figure \ref{fig:ANNSummary} shows the model with 64 nodes per layer.
The 'params' column is the result of the (inputs * weights) + bias.

The structure of the FNN model contains three dense layers (an input layer, a hidden layer, and an output layer) each with a dropout layer in between.
A dense layer is a layer where every neuron in the layer connects to every neuron in the previous layer.
Each dense layer contains an activation function.
The purpose of activation functions is covered in the 'Artificial Neural Networks' section of the literature review.
The hidden layers of the network utilise ReLU activation functions while the output uses a linear activation function.
These functions were selected based on the advice offered in the article 'How to Choose an Activation Function for Deep Learning' by Jason Brownlee, PhD\cite{machinelearningmasteryChooseActivation}.
The article states the most common activation function for hidden layers is the ReLU function due to being simple to implement and being less susceptible to vanishing gradient issues compared to other functions.
The output layer uses a linear activation function.
The linear activation function allows the model to produce an output that is a linear combination of its inputs.
This does not introduce any additional non-linearity as the goal is to predict a continuous value so there is no need to apply constraints through the activation function.

Dropout layers set the input values of a node in the layer to 0 at the specified dropout rate. 
Setting a limited number of inputs to 0 or 'dropping' them prevents the model from overfitting (becoming too familiar with the training data and losing generalization).
However, the dropout rate needs to be monitored as a dropout rate that is too high may lead to a model that trains poorly on the data.
Dropout rates are only applied during the training process to prevent overfitting, they are not applied during the testing or prediction process.

The final dense layer only contains an output shape of 1 this is the final output i.e. the predicted value of inflation given the inputs.

\subsubsection{LSTM Model Architecture}
An LSTM network is a type of recurrent neural network that is designed to excel in predicting sequential data (including time series data).
The LSTM for this project was implemented through the use of TensorFlow and Keras.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{LSTMSummary.png}
    \caption[LSTM Model Architecture Summary.]{LSTM Model Architecture Summary.}
    \label{fig:LSTMSummary}
\end{figure}
Figure \ref{fig:LSTMSummary} shows the structure of the LSTM model, including each layer's output shape, and parameters.
The input shape of the LSTM differs from that of the FNN.
This is because the FNN takes just one row of the X and y variables (X is the features being used to predict the target and y is the CPI value target to predict).
Whereas the LSTM model takes 12 consecutive rows of data, this is the equivalent of taking in a year's worth of data all at the same time.
The reasoning for this is that the LSTM model contains hidden cells that store past values in order to predict future values.
This should, in theory, improve the LSTM model's ability to predict sequential data.

The LSTM was not coded with any additional activation functions. 
This is due to the fact that the Keras' LSTM implementation already contains multiple activation functions.
This is due to the hidden state and the cell states of each LSTM cell both containing activation functions (sigmoid and tanh respectively).
The hidden state contains the memorized past data and the cell state contains the current data being given to the cell.
Thus adding an additional activation function to the output is not necessary as there is already ample non-linearity being implemented.

Furthermore, the final LSTM model did not include dropout layers as when tested these had a significant negative impact on the results produced.
This is possibly due to the fact that dropping values in an LSTM may break down the long-term relationships the model is trying to analyse.

The final layer of the LSTM is a dense layer. 
This layer serves as the output by compiling the results from all the nodes in the previous layer into a single node.

\subsection{Hyperparameter Optimisation}
Both the LSTM and FNN underwent hyperparameter optimisation in order to produce the best performance for each model.
The hyperparameter optimisation was done through a grid search technique.
The grid search method is done by exhaustively searching a manually specified set of hyperparameters.
The purpose of optimising the hyperparameters in this way is to ensure that the hyperparameters are the best possible values within the given subset.
The downside to optimising hyperparameters using a grid search technique is that it is computationally expensive.
This is because the model needs to be trained and tested on every possible combination of hyperparameters in order to ascertain which combination is optimal.
Grid search is not the only algorithm for hyperparameter optimisation, alternatives to the grid search method are a random search algorithm and a gradient descent algorithm.
Random search randomly samples combinations of parameters so it is less computationally expensive but it is unlikely to produce an optimal combination.
Gradient descent optimisation updates hyperparameters to minimise the result of a loss function but there is a chance of the algorithm getting stuck in a local minimum which may not be the global minimum of the function, thus producing a sub-optimal result.
Because of these factors, the grid search technique was chosen because it is more comprehensive than the other algorithms and is guaranteed to produce the optimal combination of hyperparameters from the given set. 

The hyperparameters that were optimised for the FNN model were the number of nodes per layer, the dropout rate, the learning rate, and the size of the batches.
For the LSTM model a smaller subset of hyperparameters were used these were the number of nodes per layer, the learning rate, and the size of the batches.\\
The dropout rate is the rate at which during training a random node will be dropped (have its inputs set to 0).
The LSTM model did not have dropout functions so there was no need to include dropout probability as a hyperparameter.\\
The learning rate dictates the step size while moving towards the minimum of the optimizer loss function. 
Both models used the ADAM optimizer provided by Keras as it is computationally efficient and does not require lots of memory (which was limited as the models were being run from a standard laptop).\\
The batch size is the number of rows of data given to the model at once, using smaller batches can speed up the training time of a model as it only has to store the error values for a subset of the data instead of all of the data.
Additionally, small batch sizes have been shown to increase the generalization performance of a model.

The set of hyperparameters tested in the grid search were as follows: 
number of nodes per layer = [16,32,64,128], dropout rate = [0,0.1,0.2], learning rate = [0.01,0.005,0.001], batch size = [8,32,64].
These values were selected as they are commonly used or default settings for these hyperparameters.
More values could be tested but this would take up exponentially more time and processing power in order to compare every combination.\\
For the FNN model, it was found that the ideal hyperparameters were 16 nodes per layer, a dropout rate of 0, a learning rate of 0.005, and a batch size of 32.\\
For the LSTM model, it was found that the ideal hyperparameters were 32 nodes per layer, a learning rate of 0.01, and a batch size of 8.

These hyperparameters were used as the default for the respective models going forward.

\section{Testing and Model Results}
\subsection{Testing Objectives}
Before beginning testing it is beneficial to understand what the objectives of the tests are.
The main objectives are: identifying any errors or bugs in the code, ensuring the results are of an acceptable standard, and identifying potential areas of improvement.
Identifying and analysing areas of improvement will be carried out further in Chapter 5.

\subsection{The Testing Set}
The testing set is the set of previously unseen data that the models will be tested on.
The set was made up of the final 30\% of the original unshuffled data.
As the testing set was not shuffled, it provides a better perspective on how the models will perform for future inflation data as it will also be sequential.

\subsection{Identifying Errors}
Throughout the implementation process, the code was continuously tested to ensure it was working as expected.
This included both unit testing (testing a single piece of code) and integration testing (testing that multiple pieces of code work together as intended).
Continuous testing was made easier by the fact that Jupyter Notebook allows the user to divide their code into cells and run the cells individually.
Being able to see the output of an individual cell makes unit testing much faster as the entire code does not need to be run.
This is particularly true for machine learning projects as training a model can often be time-consuming.

When dealing with the errors found by the continuous testing, documentation was often the first port of call.
Due to the well-known nature of the libraries this project utilised, documentation was thorough and well-written, making identifying and understanding errors much easier.
Furthermore, most of the libraries in use, such as Sklearn or TensorFlow, have active communities that help provide solutions through websites such as Stack Overflow.
This meant that many of the errors produced during the implementation of the project had already been identified and solved by other users, once again making the implementation process much smoother.

\subsection{Quality Assurance}
Another purpose of testing is to ensure that the outputs of the code are of a high enough quality regardless of whether they are the correct output or not.
This is particularly important as the value of a machine learning model rests on the quality of results it produces.
For quality assurance testing, several regression metrics were used such as MAE, RMSE, MAPE, and R-squared.
These metrics evaluate the effectiveness of the models.
The metrics were calculated using the actual testing set against the models' predictions of the testing set.
The ideal values for each metric are MAE=0, RMSE=0, MAPE=0, R-squared=1.
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    Models                         & MAE    & RMSE   & MAPE          & R2        \\ \hline
    Linear Regression              & 0.1526 & 0.2611 & 9188948002912 & -4.4515   \\ \hline
    Random Forest Regression       & 0.1040 & 0.1272 & 1949910168334 & -0.2941   \\ \hline
    Support Vector Regression      & 0.1537 & 0.1784 & 2210975015699 & -1.5471   \\ \hline
    Long Short-Term Memory         & 0.0504 & 0.0642 & 5815110419921 & 0.7082    \\ \hline
    Feedforward Neural Network      & 0.0225 & 0.0304 & 4617012820526 & 0.9207    \\ \hline
    \end{tabular}
\end{table}

Of the models shown, linear regression performs the worst and the feedforward neural network performs the best.
However, there are some strange outliers within the regression metrics.
To understand these outliers better there will be a brief overview of each metric.

MAE (Mean Absolute Error) measures the sum of absolute errors divided by the number of observations.
The value of MAE is a positive real number.

RMSE (Root Mean Square Error) is the square root of the MSE which is the average of the squared difference between actual values and predicted values.
RMSE was used as it is easier to interpret than MSE because MSE is in squared units.
An RMSE value will be a positive real number.

MAPE (Mean Absolute Percentage Error) is the average of the absolute percentage error of each observation.
MAPE values are on a scale of 0 to 1 with 0 being a 0\% deviation between the predicted and the actual values.
Yet all of the models, including the two best-performing models, produced exceptionally high MAPE values despite the fact that they would be expected to produce lower values.
The reason for this is that if values are 0 or close enough to 0, it can cause the MAPE calculation to divide by 0 which is undefined.
The Sklearn metric deals with this by producing an arbitrarily high output.
Including MAPE as a metric proves to be ineffective in this case as normalising the data between the the range [0,1] most likely caused this error.

Another of the errors appears in the r-squared collum, where it produces negative values for some of the models.
R-squared measures the proportion of the variance for a dependent variable that is predicted by an independent variable, this is also known as the coefficient of determination.
Despite what the name suggests, R-squared is not always squared, so a negative value is not necessarily an error.
Rather, the R-squared metric produces a negative value when the model's fit is worse than the fit of the mean of the data.

\subsubsection{Ensuring the Models Do Not Over Train}
Another part of quality assurance in machine learning is ensuring the models do not overtrain.
Overtraining is when a model learns the training set so well that it impacts the model's overall generalisation.
This is obviously negative, as the model's job is not to predict data it has already been given, but rather to predict data it has never seen before.
Deciding the number of epochs to train the model on requires a balance between a high number of epochs which can produce better results but will take more processing time and potentially cause overtraining.
During the implementation of the models, a few methods were used to ensure that overtraining was not happening.
The first method was producing a loss graph that tracks the loss value over the number of epochs.
The loss should gradually fall over time, however, if the loss begins to rise it can be an indication of overtraining.
%graphs
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{annloss.png}
    \caption[FNN Model Loss Over Time (epochs).]{FNN Model Loss Over Time (epochs).}
    \label{fig:annloss}
\end{figure}
By observing \ref{fig:annloss} it can be seen that the loss plateaus after around 60 epochs.
This means that the model could be trained for fewer epochs while still producing similar results and saving time.
After the loss plateaus, at no point does it begin to rise, therefore we can assume that the model has not overtrained.

The second method used to test for overtraining was by making the model predict the training data after it had already been trained.
This will test the model's performance on data it has already seen.
If it has not overtrained then it is likely that the model will not be 100\% accurate.
If the model predictes the training data with near 100\% accuracy it may have overtrained.
This method is less accurate than the previous method, however, it is quick and easy to produce after training the model.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{nntraining.png}
    \caption[FNN Model's Predictions of Seen Data.]{FNN Model's Predictions of Seen Data.}
    \label{fig:nntraining}
\end{figure}
Figure \ref{fig:nntraining} shows that the model was extremely effective at predicting the initial portion of the training data, but its accuracy tapers off towards the end of the data.
This is possibly because the final portion of the training data has much finer increments in change compared to the large movements at the start of the data.
The model is likely struggling with the change in momentum between the highly volatile first portion and the smaller changes in the second portion of the data.
However, as the model does not seem to be able to predict the training data with nearly 100\% accuracy, then it is further indication that the model is unlikely to have overtrained.

Likely due to the complexity of the data, the models could be run for a relatively large number of epochs without overtraining.
Overtraining began occurring around 300 epochs, furthermore training the models for this many epochs became increasingly time-consuming whilst using the grid method for hyperparameter optimisation.
Ultimately, the models were each trained for around 100 epochs as this seemed to be the sweet spot between saving time and producing results that are towards the higher end of what the model is capable of.

\section{Final User Friendly Model Implementation}
Although further comparisons and evaluation will be completed, based on the regression metrics taken, the feedforward neural network produced the best results for predicting inflation.
As such a more user-friendly implementation was created using this model.
The implementation consists of a checkbox list of the dataset's features.
These features can be selected and then a model will be trained and tested using the selected features.
Additionally, the number of months the X data is shifted from the target value can be altered using a slider.
If the slider is set to 12 then the dataset will be aligned so that the X data is predicting the target values 12 months in the future.
This implementation allows for more flexibility in producing predictions, however, a large majority of the predictions will be worse than the default settings of the model.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{skeleton.png}
    \caption[A Screen Shot of a Section of the Input Screen for the Adjustable Model.]{A Screen Shot of a Section of the Input Screen for the Adjustable Model.}
    \label{fig:skeleton}
\end{figure}
Figure \ref{fig:skeleton} shows part of the input section of the adjustable model.
After running these inputs the model will output a graph of the predicted values of inflation compared to the actual values along with a loss graph from the model's training.
%show screen shots


% %TODO
% - compare the results of each model
% - which was best, which was worst
% - is this the same as was predicted?
% - why did the good ones do good and the bad do bad
% - what could be improved next time?