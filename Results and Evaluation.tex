\chapter{Results and Evaluation}
This chapter covers the overall evaluation of the products developed.
This includes a comparison and evaluation of the results produced by the models as well as a reflection of what could be done differently if the project were to start over.

\section{The Models'Results}
\subsubsection{Linear Regression Model, Random Forest Regression Model, and Support Vector Regression Model Predictions}
The table of regression metrics in the previous chapter showed how the linear regression, random forest, and support vector regression algorithms all performed worse prediction methods than the mean of the data (as shown in the r-squared score).
Although this was a disappointing result, it was somewhat expected as inflation is an extremely complex variable and simpler algorithms will likely struggle to form an understanding of its properties.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{threemodelsres.png}
    \caption[A Comparison of the Predictions of Linear Regression, Random Forest Regression, and Support Vector Regression Models.]{A Comparison of the Predictions of Linear Regression, Random Forest Regression, and Support Vector Regression Models.}
    \label{fig:threemodelsres}
\end{figure}
Figure \ref{fig:threemodelsres} shows the predictions of each model compared to the actual normalised value (shown in black).
It can be seen that all three models struggle to accurately predict inflation.
In particular, around the 65th-70th month mark, all three models predict an increase in inflation where there is a decrease.
This seems to be due to a linear property indicating that inflation was likely to increase, however, possibly due to factors outside of the feature set, the increase was held off.

\subsubsection{LSTM Network Model Predictions}
According to the metric testing, the LSTM's results were an improvement upon the results of the previous three models, however, it still struggled to accurately predict inflation.
This is seen further in the graph comparing the LSTM's predictions to the actual values.
The predictions fail to predict the more erratic highs and lows of inflation, instead producing a much smoother curve.
This could potentially be due to the number of past values the LSTM is given.
As it stores 12 past values it may struggle to predict the sharp gains and losses of inflation.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Include figure for LSTM
% Further analysis if needed
% Talk about the training data as well
% over trained or not
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Aritficial Neural Network Predictions}
Of the models implemented, the custom ANN produced the best results.
This model, like the others, was implemented on a subset of the original data that did not include values from before the 1980s.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{anntotal.png}
    \caption[Predictions from the Artificial Neural-Network Trained on the Entire Dataset.]{Predictions from the Artificial Neural-Network Trained on the Entire Dataset.}
    \label{fig:anntotal}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{ann150.png}
    \caption[Predictions from the Artificial Neural-Network Trained on a subset of the Dataset.]{Predictions from the Artificial Neural-Network Trained on a subset of the Dataset.}
    \label{fig:ann150}
\end{figure}
Figures \ref{fig:anntotal} and \ref{fig:ann150} show the results produced by the model with the entire data vs the subset of the data respectively.
The figures show how the models trained on the subset of the dataset produced more accurate predictions than the models trained on the entire dataset.

\subsubsection{Training and Testing the Models on the Entire Data vs on a Subset}
% %TODO
% - talk about overfitting/training, how many epochs used and how we know it wasnt overtrianed.
% - talk about the whole set fs the set without hte first 150 points
% - compare the results of each model
% - which was best, which was worst
% - is this the same as was predicted?
% - why did the good ones do good and the bad do bad
% - what could be improved next time?