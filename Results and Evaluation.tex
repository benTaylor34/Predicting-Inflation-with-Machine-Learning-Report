\chapter{Results and Evaluation}
This chapter covers the overall evaluation of the models that were developed.
This includes a comparison and evaluation of the results produced by the models as well as a reflection of what could be done differently if the project were to start over.

\section{Results From the Univariate Implementation}
Univariate analysis was only implemented using linear regression, random forest regression, and support vector regression.
This was because it was unlikely for the results of the univariate analysis to be of a high standard due to the complexity of inflation as such it was deemed unnecessary to implement any more models.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{LinearRegUni.png}
    \includegraphics[width=0.8\textwidth]{RandomForestUni.png}
    \includegraphics[width=0.8\textwidth]{SVRuni.png}
    \caption[Results From the Univariate Implementation of Linear Regression, Random Forest Regression, and Support Vector Regression.]{Results From the Univariate Implementation of Linear Regression, Random Forest Regression, and Support Vector Regression.}
    \label{fig:Univar}
\end{figure}
As expected, the results from attempting to predict inflation solely based on its history were extremely poor.
None of the three models implemented produced predictions that could be helpful in any capacity.
These results serve to cement the complexity of inflation and how it is affected by many outside factors, thus making it necessary to use a large range of relevant features in order to produce an accurate forecast.

\section{The Multivariate Models' Results}
\subsection{Linear Regression Model, Random Forest Regression Model, and Support Vector Regression Model Predictions}
The table of regression metrics in the previous chapter showed how the linear regression, random forest, and support vector regression algorithms all produced forecasts worse than the mean of the data (as shown in the r-squared score).
Although this was a disappointing result, it was somewhat expected as inflation is an extremely complex variable and simpler algorithms will likely struggle to form an understanding of its properties.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{threemodelsres.png}
    \caption[A Comparison of the Predictions of Linear Regression, Random Forest Regression, and Support Vector Regression Models.]{A Comparison of the Predictions of Linear Regression, Random Forest Regression, and Support Vector Regression Models.}
    \label{fig:threemodelsres}
\end{figure}
Figure \ref{fig:threemodelsres} shows the predictions of each model compared to the actual normalised value of inflation (shown in black).
It can be seen that all three models struggle to accurately predict inflation.
Interestingly, around the 65th-70th month mark, all three models predict an increase in inflation where there is a decrease.
This could be due to a linear property indicating that inflation was likely to increase, however, possibly due to factors outside of the feature set, the increase was held off.

It was expected for linear regression to struggle in predicting inflation as the relationship between it and the other features is complex.
This was shown in the correlation matrix created that showed very little Pearson pairwise correlation between CPI and the other features.
Despite this, the linear regression model's predictions still outperformed random forest regression and support vector regression if the large outlier around month 70 is ignored.

Random forest models seemed to have a good reputation for predictions during the literature review yet still failed to provide an accurate prediction for inflation.
This is possibly due to the fact that each tree in the random forest model is trained on a random subset of the data.
This may not be a good method for predicting inflation as it is sequential data so training random sections of the data on different trees likely removes any temporal dependencies present in the original data.

Support Vector Regression (SVR) predicts data by creating a hyperplane.
However, it is difficult to fit a hyperplane on data with a high number of dimensions or lots of noise.
Both of these caveats apply to the dataset used to predict inflation as it contained many different features each likely to have noise of some kind such as price fluctuations.
Furthermore, although SVRs create a hyperplane they are still made to work on linearly separable data so if the data contains too many complexities and non-linear relationships then SVRs can encounter issues in providing accurate predictions.

Overall, these three models are likely inappropriate use for predicting inflation when using a complex dataset.

\subsection{LSTM Network Model Predictions}
According to the metric testing, the LSTM's results were an improvement upon the results of the previous three models, however, it still struggled to accurately predict inflation.
This is seen further in the graph comparing the LSTM's predictions to the actual values.
The predictions fail to predict the more erratic highs and lows of inflation, instead producing a much smoother curve.
This could potentially be due to the number of past values the LSTM is given.
As it stores 12 past values it may struggle to predict the sharp gains and losses of inflation.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{lstmtotal.png}
    \caption[Predictions from the LSTM Network Trained on the Entire Dataset.]{LSTM Network Trained on the Entire Dataset.}
    \label{fig:lstmtotal}
\end{figure}
The predictions from the model trained on the entire dataset, produce a smooth curve that follows the trends of inflation but fails to depict the magnitude and precision of the finer changes.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{lstm150.png}
    \caption[Predictions from the LSTM Network Trained on a subset of the Dataset.]{LSTM Network Trained on a subset of the Dataset.}
    \label{fig:lstm150}
\end{figure} 
The predictions of the LSTM trained on the subset of the data more accurately depict the small volatile increments but still fail to predict the magnitude of inflation.

The graph produced by the LSTM that was trained on the subset of the dataset is preferable to the graph produced by the LSTM trained on the entire dataset.
However, neither model produces close accurate predictions of the actual value of inflation.
Both LSTMs can follow the trends of inflation (although somewhat delayed) but struggle to produce predictions that are of the correct magnitude.

\subsection{Feedforward Neural Network Predictions}
Of the models implemented, the FNN produced the best results.
This model, like the others, was implemented on both the whole dataset and a subset of the original data that did not include values from before the 1980s.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{anntotal.png}
    \caption[Predictions from the Feedforward Neural-Network Trained on the Entire Dataset.]{Neural-Network Trained on the Entire Dataset.}
    \label{fig:anntotal}
\end{figure}
The predictions from the model trained on the entire dataset start well but lose accuracy towards the end of the testing set.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{ann150.png}
    \caption[Predictions from the Feedforward Neural-Network Trained on a subset of the Dataset.]{Neural-Network Trained on a subset of the Dataset.}
    \label{fig:ann150}
\end{figure} 
The model trained on the dataset that omitted the first 150 samples produced results that were of similar accuracy to the model trained on the entire dataset.
However, the benefit of the model trained on the subset was that its results maintained their consistency for the entire testing set.

Figures \ref{fig:anntotal} and \ref{fig:ann150} show the results produced by the model with the entire data vs the subset of the data respectively.
A comparison of the figures shows how the model trained on the subset of the dataset produced more accurate predictions than the model trained on the entire dataset.

\subsection{Analysing Why the Feedforward Neural Network Had the Most Success}
The artificial neural network shown in figure \ref{fig:ann150} produced the best results out of all of the models implemented to predict inflation.
This is likely due to several factors.

\subsubsection{Removing Data Outliers}
This method was applied to all models successfully.
The first 150 samples of CPI contained large outliers compared to the rest of the dataset.
Including these samples would have negatively affected the model's predictive abilities.
This was confirmed by the fact that the other models also showed an increase in performance after the first 150 rows of data were removed.

\subsubsection{Hyperparameter Tuning}
Another reason for the FNN's success is that it was the most flexible of the models with the largest number of hyperparameters that could be adjusted.
The only other model that underwent similar hyperparameter tuning was the LSTM and interestingly the hyperparameters that were found to be optimal were similar for both models.  
However, some of the hyperparameters found to be optimal by the grid search were somewhat unexpected.\\
The model performed best with 128 nodes in the hidden layer, which was the highest option in the grid and higher than expected.
The reason for the model preferring a higher number of nodes could be that with more nodes the model could better capture the problems' more intricate non-linear patterns.
However, such a large number of nodes comes at a computational cost as it will require more processing power and take more time to run the model.\\
The preferred dropout rate was 0\% meaning that it is quite unlikely the model came close to overtraining as if it did the model would have been more successful with a higher dropout rate.
Another possibility is that the other dropout rates available in the grid were already too high and dropouts were occurring too often.
The next smallest dropout rate available was 10\% or 0.1 so this theory is unlikely as 0.1 is considered a relatively low rate.\\
The ideal learning rate was 0.005 which was unexpected as it is higher than Keras' default learning rate of 0.001.
A higher learning rate often allows for faster convergence but can lead to the model oscillating or diverging if it overshoots the optimal point.
Despite this, it does not seem like the model oscillated or diverged based on the loss graph as it did not display any additional inflections.\\
The final hyperparameter was a batch size of 8, this was the smallest batch size option.
The batch size dictates the number of data samples the model works through before it updates its internal parameters.
Thus a smaller batch size means that the model's parameters are being updated more frequently.
Research\cite{masters2018revisiting} indicates that smaller batch sizes produce models with better generalisation.
This can even be true for models trained using mini-batch sizes as small as 2.
A model that updates its parameters more frequently is likely to perform well on data that has a lot of complex relationships.
Therefore, the fact that the smallest batch size produced the best results is not surprising as the data the model is training on likely contains many such relationships.\\

\subsubsection{Recurrent Neural Networks vs Non-Recurrent Neural Networks}
The previously mentioned factors still do not explain why the feedforward network produced superior results to the LSTM.
During the 'Expected Model Performance' section of this report, the LSTM had been predicted to produce the best results.
This prediction was made on the basis that an LSTM has the ability to retain historical data and use it in conjunction with present data in order to tune its parameters.
Having this ability should in theory make the LSTM better suited for utilising the sequential time series data that made up the dataset for this project.
However, as shown in the results of both the metrics and graphs presented, the LSTM performed worse than a non-recurrent network: the feedforward neural network.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{annvslstm.png}
    \caption[A Comparison of the LSTM and FNN Predictions.]{A Comparison of the LSTM and FNN Predictions.}
    \label{fig:annvslstm}
\end{figure} 
One explanation for the poor performance of the recurrent neural network is covered in a quote from Justin Sirignano and Rama Cont, which states:
"financial data can be 'non-stationary' and prone to regime changes which may render older data less relevant for prediction"\cite{sirignano2018universal}.
This means that outside factors can have an effect on the value of financial data.
In the context of this project, this could mean that outside variables that were not included in the feature set impacted inflation which would lead the LSTM to struggle in predicting inflation's value.
However, if this was the case then the feedforward network should have struggled to form accurate predictions in certain areas as well.
Furthermore, the features in the dataset cover a wide range of macroeconomic indicators that should exhibit changes in the case of an outside event occurring.
Finally, there was no obvious indication in the graph comparison between inflation and the LSTM's results that a regime change had taken place.
A regime change would have been represented in dips or spikes in inflation that were not represented in the LSTM's results. 
So although it may be a contributing factor, it is unlikely that the sole reason for poor performance from the LSTM is due to outside features.

Another explanation is that the problem of forecasting inflation may be less sequential than it was initially thought to be.
LSTMs are made to forecast sequential data and time series data, but what if the value of inflation does not rely heavily on past values?
It may seem a strange hypothesis but it could potentially mean that each value of inflation is independent of the past values, within reason.
LSTMs are a popular choice for a recurrent neural network because of their ability to recognise long-term dependencies, but if inflation's value does not rely heavily on long-term temporal dependencies then this ability will not be useful.
Furthermore, an LSTM's recurrent nature can impede its performance if the target does not show any patterns relating to long-term dependencies.
LSTMs take longer to train than standard feedforward networks and require more computational power due to their recurrent nature.
Using non-sequential data on an LSTM may also hinder its predictive ability as LSTMs assume long-term relationships exist in the data causing the model to look for relationships that do not exist.

Sirignano and Cont solved all of the performance problems of their LSTM with one solution: lots of data.
A quote from their paper states: "The resulting LSTM network involves up to hundreds of thousands of parameters... ...it is huge compared to econometric models traditionally used in finance"\cite{sirignano2018universal}.
In comparison the LSTM implemented in this project involved a mere 15000 total parameters, this massive disparity could be the reason the LSTM in this project did not perform as well.
It seems that for an LSTM to perform to the best of its ability a large amount of data is needed.
More data means more long-term relationships and more long-term relationships means better performance from recurrent neural networks.

Thus, standard feedforward neural networks may be better suited for smaller datasets as the temporal relationships are less prominent in a limited dataset.

\section{Technical Difficulties}
From the beginning to the end of this project, constant hurdles were faced and overcome.
This section will discuss some of the technical difficulties presented by the task of 'Predicting Inflation with Machine Learning'.

\subsubsection{Difficulties in the Literature Review}
The literature review presented several unique difficulties.
Most of all was the sheer amount of literature available.
The task of reviewing a topic with so much variety and content was seemingly insurmountable.
This posed the age-old question of where to start.
Survey papers were a blessing by providing the results of several papers compiled into one.
Thanks to the obvious monetary incentive of being able to predict financial time series like stocks there was a plethora of papers available covering topics not dissimilar to this project.
However, there was a rather distinct lack of crossover between machine learning and inflation forecasting.
This could be possibly because it is the job of national banks to monitor inflation and they are often large enough and have enough resources available to conduct in-house research.

Reviewing literature ultimately became a constant throughout the project causing many changes and improvements to be made every time new information came to light.
If this project were to be completed again it would be a much smoother process purely because of the literature that has already been covered both in and out of this report and the knowledged gained from it.

\subsubsection{Difficulties in the Design}
The design chapter posed challenges of its own, chief of which was data selection and preprocessing.
The choice was made to include data selection and preprocessing in the design section as the implementation of the models could not be started without an already suitable dataset.
Data selection was particularly difficult as inflation is commonly collected annually and collected quarterly at best.
This would not be enough data samples to adequately train a machine-learning model.
Go-to dataset sites like Kaggle provided no solution to this problem.
Even both the Bank of England and the World Bank did not possess adequate data in this regard or at least it was not available on their websites or as part of their online databases.
For a long period, it seemed like the project was doomed to poor data and poor data produces poor models. 
Eventually, OECD's CPI data was found which was collected monthly.
Although, this too was not as straightforward as it should have been as not only did OECD not store its CPI data with the rest of its country indicators but OECD also did not store its publicly available databases in a tabular format.
This meant that it was necessary to undergo the painstaking task of converting hundreds of samples of data into an admissible format.
Despite the stress of having to navigate the websites of multiple million-dollar institutions that seemed to be purposefully built like mazes created to withhold information, the best that could be found was data collected monthly.
This meant that there were only a few hundred data samples available.
To add insult to injury, a large portion of CPI data (the first available 10 years) was disproportionately large: it was an outlier.
As such, although removing it increased the performance of the models, it left the data set with a minor 500+ samples: likely too few to produce the best possible results.
Fortunately, despite the undersized dataset, a few of the models were still able to produce passable predictions.
If there was only one thing that could be improved about the project it would be the size and quality of the dataset.

\subsubsection{Difficulties in the Implementation}
The issues encountered with the sample size naturally followed through into the implementation process, contributing to subpar results from the models compared to if they had been trained on more data.
The small sample size meant that the training and testing set sizes were limited.
A model trained on a smaller training set is likely to produce worse results than a model trained on lots of data.
Furthermore, using a small testing set could cause performance estimates and metrics to be unreliable.
Thankfully despite these issues, both the feedforward network and the LSTM were able to produce passable results.
All of the models would have likely benefitted from being trained on a larger dataset.
As inflation data and many other economic indicators are historical, they can only be collected so many times each year.
This means that as the years progress more and more data will be produced but the rate of collecting new data cannot be increased.
There is the potential for using tools like autoencoders to generate new data\cite{Bungaro2020} but this may cause models to produce unreliable results.
As such, the issue of limited data for macroeconomic problems is likely to persist for a long time to come. 

Another difficulty encountered during the implementation process was in attempting to maintain the temporal consistency of the data.
As inflation is a time series, shuffling the data - as is common practice- during the training and testing split was not applicable.
Shuffling the data would have meant that the results produced are difficult to interpret and are likely not applicable for real-world use as inflation will always be a time series.
Furthermore, several programming errors were encountered when trying to shift the data in order to maintain its time consistency.
Data needed to be shifted as using values from the same year to forecast other values from that year would be fruitless.
This is because OECD and many other institutions release all of the indicators simultaneously every month so the target value would already be available.
As such the data had to be altered so that the models would use data from the previous year to predict the target values of the current year.
The programming errors encountered when implementing this were eventually resolved, however, the solution involved resetting the index of the dataframes and dropping even more data.
Thus the further into the future the user wishes to predict, the fewer samples will be available in the dataset.
Predicting one year into the future did not cause much issue as only twelve samples had to be dropped (one for each month) yet this issue is still likely to reduce the performance of the models.

The final issue that will be discussed regarding the implementation of this project is one that is encountered in almost every machine learning project: the runtime.
This was especially prominent when utilising the grid search method for hyperparameter optimisation as it meant training and testing the models on every combination of hyperparameters in the given set.
Using a smaller set of hyperparameters would reduce the wait time but it may also produce worse results if the optimal hyperparameters were not included in the search set.
Once the optimal hyperparameters of each model were found they were fed directly to the models for training and testing moving forward in order to avoid using the grid search every time a model had to be run.
Using a more powerful computer could help combat this issue or using an online service such as Google Collab that provides free access to graphics processing units (GPU) and tensor processing units (TPU) that aid in reducing the execution time of programs.


% - waht problems were faced? how were they overcome?
% - what would you do if u had more time/ next time
% - what would you do differently
% - what have you learnt and experienced


% %TODO
% - is this the same as what was predicted
% - what could be improved next time?

