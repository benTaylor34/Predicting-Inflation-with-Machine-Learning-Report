\chapter{Design}
The design process for machine learning algorithms varies depending on the size, complexity, and use cases of the algorithm.
According to Chip Huyen, author of the book "Designing Machine Learning Systems"\cite{Huyen_2022}, the machine learning project flow has four distinct steps:
project setup, data pipeline, modeling and training, and finally: serving.
These four steps can be iterated through as seen in figure \ref{fig:machineLearningDesignPipeline}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{machineLearningDesignPipeline.png}
    \caption[Machine Learning Design Pipeline by Chip Huyen]{Machine Learning Design Pipeline by Chip Huyen. \par \textit{Figure from https://huyenchip.com/machine-learning-systems-design/design-a-machine-learning-system.html}\cite{Huyen_2022}}
    \label{fig:machineLearningDesignPipeline}
\end{figure}
This is the project flow that will be followed for this project and throughout this design chapter.
 
\section{Project Setup}
The project setup step of Heung's design flow mainly focuses on outlining the details of the project.
Examples of these details are the project's: goals, user experience, evaluation, personalization, and constraints.
This report's project setup shall focus mainly on the goals, evaluation, and constraints sections as user experience and personalization are not key aspects of the project.

\subsection{Goals}
As the title of the project outlines, the goal of the project is to predict inflation using machine learning.
This project will evaluate the use of various machine learning methods and assess their ability to predict inflation.
Further aims and objectives are outlined in Chapter 1. Introduction.

\subsection{Expected Model Performance}
As multiple models will be implemented in this project, it is unlikely that all of the models implemented will perform to the same standard.
Thus it is important to have a rough prediction as to which models will perform better or worse.
By making these predictions it will prevent additional time from being wasted on optimizing or making changes to models that show less potential.
This does not mean that these models should not be implemented properly but rather it means that the limited time for implementation should focus on the models with the best potential to predict inflation.
Although all the models that will be implemented hope to be able to predict inflation to some degree, certain models have a higher likelihood of better performance.
Five models will be implemented: a linear regression model, a random forest model, a support vector regression model, a long short-term memory network model, and a feedforward neural network.
Further details on the models that shall be implemented will be covered later.
The models that are predicted to have the best performance are the long short-term memory network (LSTM) and the feedforward neural network (FNN).
This is because of their increased complexity and the fact that they can be modified more easily, giving more control over the architecture of the model.
Of these two models, the LSTM is predicted to perform the best due to the fact it can memorise previous data which should help the LSTM to better analyse the patterns of a sequential time series such as inflation.
The models that are predicted to perform worse are the linear regression model, the random forest model, and the support vector regression model.
This is in view of their simplicity and the fact that inflation is a complex variable to predict and so is likely to require more complex models to create an accurate prediction.
This means that throughout the design and implementation, more time will be allocated to tweaking and improving the performance of the LSTM and FNN models as they are predicted to have a higher potential.

\subsection{Evaluation}
Evaluation outlines how the system's performance shall be evaluated: what metrics shall be used, what visualisations shall be produced, and so on.
The models created in this project will be regression models attempting to predict an exact value of inflation.
Thus the methods selected to evaluate the models should be suited for regression and continuous values.
This means that methods like confusion matrixes that are better suited for classification would not be a good choice.
As stated in the literature review, the most common regression metrics were MSE, MAE, MAPE, and R/ RÂ².
As each of these metrics is straightforward to implement, they will all be used for evaluating the models created.
Furthermore, visualisations displaying each model's predictions alongside the values they were attempting to predict will be produced.
This will help to understand the model's effectiveness beyond numerical metrics.
Where appropriate, certain models should produce loss graphs as these will be able to guide hyperparameter optimisation, such as selecting the correct number of epochs to train a model.

\subsection{Constraints}
The Constraints of a project can follow two veins: performance constraints and project constraints.
Performance constraints address how a project must produce its results. 
Examples of performance constraints are how fast predictions should be made and how precise predictions should be.
Of these two constraints, this project's main concern is with precision.
The predictions of each model created will be evaluated and compared to one another in order to conclude the type of machine learning model best suited to predicting inflation.

On the other hand, project constraints are real-world constraints that may limit the project such as the time or manpower available.
A typical project constraint is the software available.
However, since it is common practice to use Python for artificial intelligence research (even for billion-dollar companies such as Google\cite{Google}) and Python is a free-to-use, open-source language this particular constraint should not be an issue.
The constraints of this project are mainly generic constraints that will apply to most final-year projects.
One such generic constraint is the fact that only one student will be working on the project and the student has several other modules to focus on throughout the year.
This means that a limited amount of time and resources will be dedicated to the project.
In the context of predicting inflation with machine learning, this means that it is extremely unlikely that any of the models produced by the end of the project will be more capable or effective than current models in use from large organisations such as the Bank of England.
As these organisations will have dedicated more time, manpower, and resources toward producing models with greater sophistication and accuracy than those that could be created by a lone student.
Even if models of similar complexity were to be created from this project, there would not be the hardware available to consistently and reasonably train and test such models.
These factors mean that given the task of predicting inflation with machine learning, it is more suitable to create numerous models and compare their effectiveness rather than creating a single extremely sophisticated model that would require more resources to produce.

\section{Data pipeline}
The data pipeline section of designing machine learning algorithms deals with the selection and preprocessing of data.

\subsection{The Systems Input and Output}
One of the first questions that needs to be addressed is: "What is the system producing and from what information are these predictions being produced?"
The input is real-world historical economic data relating to UK inflation. 
The output is an array of values predicting future inflation.
As all of this data is publicly available the project should have very few privacy and bias concerns.

\subsection{Data Selection}
The saying "garbage in garbage out" succinctly illustrates the importance of selecting good data for machine learning models.
It does not matter how powerful a model is if the data selected is poor or inappropriate.
Selecting data is a key step to building effective machine-learning models.
Thankfully, there are plenty of large open-source data sets available online, despite how time-consuming and expensive gathering data may be.
Yet picking an appropriate set may still present a challenge.
The criteria for a dataset for this project are as follows:
\begin{itemize}
    \item A large number of data points (preferably in the thousands) as inflation is a complex feature to predict. 
    \item Several related economic indicators to use as features for predicting inflation and to test their ability to predict inflation.
    \item Minimal missing or erroneous data as this data can lower a model's accuracy.
    \item Reliable data - the data should be taken from a reliable source in order to accurately predict real inflation.
\end{itemize}
During the project, several datasets were tested, for example, various Kaggle datasets and the "World Development Indicators" by the World Bank \cite{WBI}.
The dataset that was finally settled on was the UK financial statistics from OECD (The Organisation for Economic Co-operation and Development)\cite{OECD}.
The reason for selecting this set is due to its large variety of indicators, the reliable provider, and the fact that it is open-source 
(the dataset is classified as public under the access to information classification policy). 

Initially, datasets from Kaggle were tested but they lacked a sufficient number of data points for multivariate machine learning analysis of inflation.
Data from the World Bank was also preprocessed and tested with basic models.
The World Bank is known for reliable and accurate data which should avoid issues of bias, or poor quality.
However, the World Bank sourced a majority of its indicators annually dating back to the 1960s meaning that there would only be around 60 rows of data up to the present day.
Thus, although the set had a large number of interesting features, the number of data samples was insufficient for a machine-learning model.
Additionally, due to the large number of features (over 1400), it would need a sufficient amount of cleaning/preprocessing in order to remove irrelevant features.

\subsection{The Final Dataset}
Due to the issues encountered with the other datasets, the OECD dataset was selected for use alongside monthly CPI data sourced from the World Bank.
The first main benefit of the OECD dataset is that the samples are taken monthly so there are a lot more data points than datasets that collect annually.
The features from the OECD dataset were taken from 1972 to 2022.
Unfortunately, a few of the features had less data as they were taken from later than 1972 with the latest being taken in 1987. 
However, as this is only a small fraction of the overall dataset, replacing the missing values with zeros is unlikely to have a profound effect on the final results.
The second benefit is that the dataset contains a moderate number of features to choose from (not too many that the features need to be drastically reduced (as was the case with the World Bank dataset)).
There were twenty different features in total (not including CPI), these features were: 
composite business confidence, composite consumer confidence, composite leading indicator (CLI), consumer prices, hourly earnings, immediate interest rates call money interbank rate, long-term interest rates, M1, M3, merchandise exports, merchandise imports, nominal exchange rates, passenger car registrations, production volume, GDP, retail trade volume, share prices, short-term interest rates, and unemployment.
Histograms were plotted for each feature in order to assess their distributions.
Understanding the distribution of a feature can give some insight as to whether to use normalisation (rescaling the data between two set values) or standardisation (setting the mean of the data to 0 and the standard deviation to 1) before giving the data to the machine learning models.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{hist.png}
    \caption[A Histogram of Each Feature in the Dataset.]{A histogram of each feature in the dataset}
    \label{fig:hist}
\end{figure}
The histograms display a variety of different distributions, this is understandable as many time series do not follow a normal distribution.
The distributions indicate that normalisation may be preferable to standardisation as normalisation is preferable when several features do not have a Gaussian (normal) distribution.
This is because normalisation will preserve the distribution of the data by rescaling the data where as standardisation makes the assumption that the data has a Gaussian distribution and may modify the data's distribution if it is not already Gaussian.
As the distribution of the dataset will remain non-normal, some precautions may need to be observed when using models that assume a normal distribution (such as linear regression).

In addition, a correlation matrix was formed for all of the features to determine if any features were overlapping.
A correlation matrix gives us an indication of how the features relate to each other one on one.\\
\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{corr.png}
    \caption[A Correlation Matrix for the Features in the Dataset.]{Correlation Matrix for the Features in the Dataset}
    \label{fig:corr}
\end{figure}
Although, at first glance, the correlation matrix seems to show very little correlation between the features, this does not mean that the features are not correlated or cannot be used for predicting inflation. 
This is due to the fact that the heatmap figure was generated using a pairwise Pearson correlation function.
Although pairwise Pearson correlation can often give a good representation of the correlation of two features it may not always be accurate.
Pearson correlation attempts to draw a line of best fit between two features, meaning that if the two features do not share a linear correlation then Pearson correlation may produce a poor result.
As the relationship between inflation and its causes is extremely complex, with many different factors contributing to the value of inflation in varying amounts, it is unlikely that inflation will have a strong linear correlation with the features that contribute to it.
So despite the fact that the results of the heatmap were not ideal, this does not mean that the features are inappropriate to use in our models.

\subsection{Data Pre-processing and Transformation}
Often, datasets have several outstanding issues or properties that make them imperfect for use in a machine-learning model.
Data pre-processing often includes steps such as data cleaning (handling outliers, noise, or missing values) and data integration (combining data from multiple sources). 
Data transformation is the task of molding the data to an appropriate size and dimensionality.
Both of these processes are to create a dataset that is formatted in a manner that suits our models and objectives.
Poorly processed data can be difficult for both machines and humans to understand and can lead to poor results. 

\subsubsection{Issues With The OECD Dataset}
There were some outstanding issues with the original dataset taken from OECD and thus several steps were taken to clean the data.
These issues included:
\begin{itemize}
    \item Features containing missing data.
    \item Incorrect format of the dataset (required transformation).
    \item Duplicate features.
    \item Unnecessary/erroneous data.
    \item A large amount of metadata that needed to be removed.
\end{itemize}
Any data points that fell under these issues were removed/cleaned and all null values were replaced with zeros to improve readability.
The dataset was also reshaped into a tabular format to improve readability and use in the machine learning models.

Finally, a Granger causality test was carried out on the data to gather a better understanding of our data and its correlation to the target feature (CPI).

\subsubsection{Granger Causality}
Granger causality is a statistical concept in economics used to show if time series A is useful at forecasting time series B.
Clive Granger originally proposed the test in 1969 in the article "Investigating Causal Relations by Econometric Models and Cross-spectral Methods"\cite{GCTest}.
The test only shows predictive causality and not true causality.
Additionally, the test only provides information about forecasting ability and not the actual causal relationship.
Another issue with the use of the Granger test in the context of inflation is that the test works best on stationary data.
Whether inflation is best treated as stationary or non-stationary data is currently inconclusive\cite{inflationStationaryOrNot}.
But for our purposes, these limitations are fine as we only want to understand an indication of how useful the data will be for inflation forecasting.

\subsubsection{Granger Testing The Data}
With two indicators X and Y, X causes Y if a series of tests on lagged values of X produce a p-value of less than 0.05.
The closer the p-value is to zero the more likely it is for X to granger cause Y.
Lagged values are values from a time series shifted forwards or backward in time.
In the case of the Granger test, Jeffery Woolriddge proposes that fewer lags should be used for annual data compared to quarterly or monthly data in order to not lose degrees of freedom\cite{wooldridge2009introductory}.
The Granger test was run on the preprocessed indicators comparing each of them to CPI.
The function ran 12 lags to see if data has a potential causal relationship with inflation within the past year.
This means that if at some point during the last 12 lags a p-value$<$0.05 was produced the data will be labeled as having the potential to be useful in forecasting CPI.
Of the 20 features tested, 9 produced results that indicate Granger Causality.
These 9 were: Composite business confidence, Composite consumer confidence, Consumer prices, Long-term interest rates, Merchandise imports, GDP, Retail trade volume, Share prices, and Unemployment.
This indicates that almost half of the features have the potential to predict CPI.
This, however, does not mean that the 9 features found to Granger Cause CPI definitely cause inflation as the Granger Causality test is only a measure of the potential of one value to cause another.
Furthermore, just because a value does not Granger Cause inflation does not mean that the feature is not useful in predicting inflation. 
Thanks to the Granger Causality test we now have a better understanding of the potential our dataset has to predict inflation. 
% \begin{figure}[H]
%    \centering
%    \includegraphics[width=0.75\textwidth]{GrangerTestFunction.png}
%    \caption[The function written to Granger test the data.]{The function written to Granger test the data.}
%    \label{fig:GrangerTestFunction}
% \end{figure}

\section{Modelling and Training}

\subsection{Model Selection}
The models that will be compared for their ability to predict inflation are a random forest model, a support vector regression model, a linear regression model, a long short-term memory (LSTM) model, and a feedforward neural network (FNN) model.
Most of the models were discussed during the literature review and will be implemented due to their common use in similar regression models.

Linear regression was added to the list of models to be implemented as it is an extremely simple model that focuses on plotting a single line of best fit through the data.
Including linear regression will provide a comparison to the results of a simple model, as well as potentially indicate any unexpected linear relationships between inflation and any of the dataset's features.

A long short-term memory network will also be implemented. 
LSTMs are a type of recurrent neural network (RNN). 
As mentioned in the literature review, the architecture of RNNs enables them to preserve an internal memory state that helps them to model temporal dependencies in the data.
This characteristic means that RNNs excel at processing sequential data such as time series data.
The LSTM was selected as the RNN model that will be implemented as they are considered to be one of the most powerful implementations of an RNN and are used to find patterns in numerical sequence data\cite{Nicholson_2023}.
As inflation is time series data, an LSTM could potentially provide good results.

Several of the models selected have implementations provided through the use of libraries such as Scikit-learn\cite{scikit-learn}.
Implementing a model through a high-level library lets the user create an already well-documented and tested model with only a few lines of code.
This saves both time and resources (as the model implemented from the library is likely to have been optimised) while also providing additional support, if necessary, from the community and documentation.

\subsection{Model Training}
The models will be created using Python and several of its supported libraries and written in a Jupyter Notebook.
The reason for using a Jupyter Notebook is that the cell structure allows for the steps to be run separately and the results to be displayed in real time without the entire file being run.
This makes experimenting with hyperparameters and training models much more convenient.
As the results of each cell are displayed separately this also improves the ability to quickly create and display visualisations through tools such as Matplotlib \cite{Hunter:2007}.
Jupyter also supports multiple languages and libraries enabling users to work in the preferred environment.

\section{Serving}
Serving is the process in which the models and their results are presented to the end user.
As this is an academic project this report will act as the presentation method.
This means that both the model and their results need to be suitably presented within a report.
This will be done through the use of graph visualisations created in Matplotlib. 
These visualisations should display and compare the model's predictive ability along with their learning rates.
This will be done by creating a graph for each model that displays their predictions compared to the actual values of inflation, indicating how well they can predict both training and testing data.
Loss graphs will be used to show a model's learning rate over time.